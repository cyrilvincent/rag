Le risque opérationnel pour les établissements financiers (banque et assurance) est le risque de pertes directes ou indirectes dues à une inadéquation ou à une défaillance des procédures de l'établissement (analyse ou contrôle absent ou incomplet, procédure non sécurisée), de son personnel (erreur, malveillance et fraude), des systèmes internes (panne de l'informatique…), des risques externes (inondation, incendie…) ou émergents. Dans le cadre du dispositif Bâle II, la définition du risque opérationnel, les procédures à mettre en place pour le limiter et les méthodes de quantification ont été normalisées. L'objectif de ce dispositif, mis en place dans les banques européennes en 2008, est d'éviter le risque systémique.
Les risques opérationnels ont pris une importance considérable dans le contexte bancaire né de la dérégulation, de l'imbrication croissante des acteurs du monde financier, de l'augmentation des capitaux manipulés et de la sophistication des produits comme l'ont montré les affaires Barings et Société Générale.
Dans le cadre du dispositif Bâle II ont été définies les bonnes pratiques à mettre en place par chaque établissement financier. Le régulateur financier national est chargé de les évaluer et de les contrôler. Les établissements financiers peuvent opter pour un dispositif d'évaluation de ces risques plus ou moins sophistiqué. Depuis la réforme Bâle II, le risque opérationnel entre dans le calcul des fonds propres réglementaires de l'établissement bancaire avec une incidence proportionnelle à la qualité de ses procédures et de son dispositif de suivi et d'évaluation.
Des mesures similaires à celles mises en œuvre dans le cadre de Bâle II sont en cours de définition pour les compagnies d'assurance dans le cadre de la réglementation Solvabilité II.
Le régulateur du dispositif Bâle II définit le risque opérationnel comme celui de pertes directes ou indirectes dues à une inadéquation ou à une défaillance des procédures, du personnel et des systèmes internes. Cette définition inclut le risque juridique; toutefois, le risque de réputation (risque de perte résultant d'une atteinte à la réputation de l'institution bancaire) et le risque stratégique (risque de perte résultant d'une mauvaise décision stratégique) n'y sont pas inclus.
Cette définition recouvre notamment les erreurs humaines, les fraudes et malveillances, les défaillances des systèmes d'information, les problèmes liés à la gestion du personnel, les litiges commerciaux, les accidents, incendies, inondations.
Le Comité de Bâle a retenu une classification qui institue sept catégories d'évènements liés à ce risque :
Depuis le milieu de la dernière décennie, les connaissances en matière de risques de crédit et de risques de marché ont alimenté un large débat et ont fait l'objet de très nombreux travaux de recherche. Normalement, ces travaux auraient dû contribuer à des progrès significatifs dans l'identification, la mesure et la gestion des risques au sein du système bancaire. Néanmoins, on ne peut éviter de s'interroger sur l'impact effectif de ces contributions, au vu d'événements récents qui ont exercé une influence déterminante sur la crise financière de 2008 : d'une part, la crise des subprimes, d'autre part, les pratiques des agences de notation financière dont l'intervention est déterminante dans le processus de maîtrise des risques de crédit.
Cela étant, au cours de la même période, l'évolution des marchés financiers, caractérisée notamment par la globalisation des activités bancaires et par leur dérégulation, a rendu ces activités — et donc les profils de risque correspondants — de plus en plus complexes.
Les régulateurs financiers se sont également rendu compte que les risques devenaient de plus en plus difficiles à identifier du fait qu'ils étaient présents à tous les niveaux d'une organisation, de plus en plus difficiles à mesurer de par la conjonction de pertes directes et de pertes indirectes beaucoup plus délicates à quantifier, et de plus en plus difficiles à gérer de par l'organisation de plus en plus transverse des métiers de la banque et de par les difficultés à bien maitriser les limites de leurs périmètres.
C'est en partie pour ces raisons que tant les régulateurs que les institutions bancaires ont mis en place des moyens pour identifier, mesurer et contrôler les risques opérationnels : des événements comme ceux qui se sont produits à New York en septembre 2001, ou encore la série de fraudes survenues dans des institutions bancaires (Société Générale, Barings, pour ne citer que les plus médiatisées), démontrent bien que la gestion des risques bancaires va bien au-delà des domaines des risques de crédit ou des risques de marché, et nécessite la prise en compte des risques opérationnels.
Pour la détermination des fonds propres réglementaires, qui constitue un des éléments clés de tout système de régulation bancaire, le dispositif Bâle II établit de nouvelles règles qui prennent mieux en compte la réalité économique, en affinant l'évaluation du profil de risque des institutions bancaires et en y intégrant des systèmes de mitigation des risques. Cette nouvelle régulation permet aux banques qui répondent à certaines conditions de réduire leurs exigences de fonds propres réglementaires, sous réserve d'être capables de démontrer une organisation interne efficiente dans la gestion de leurs risques.
Pour affiner la gestion et la maîtrise des risques, le ratio McDonough, remplaçant le précédent ratio Cooke, impose aux banques d'affecter une partie de leurs fonds propres à la couverture de leurs risques de crédit, de leurs risques de marché et − nouveauté du ratio McDonough − de leurs risques opérationnels.
Pour évaluer l'exposition d'un établissement bancaire aux risques opérationnels, le Comité de Bâle propose trois approches par ordre croissant de complexité et de sensibilité au risque :
Même si le calcul des exigences de capitaux réglementaires est relativement simple dans les deux premières approches (approche de base et approche standard), le coefficient de pondération étant fixé par l'autorité de contrôle, l'utilisation de l'approche standard ou à fortiori celle de l'approche avancée est soumise à une acceptation de l'autorité de contrôle, elle-même conditionnée par le respect de certains critères d'éligibilité :
« Comme pour le risque de crédit, plus les outils de gestion sont performants, donc plus l'approche est sophistiquée, moins grande sera l'exigence de fonds propres. Lorsque les conditions requises pour l'usage d'une méthode sont réunies, la banque est encouragée à l'utiliser. Une banque internationale active, et les banques ayant des risques opérationnels significatifs, sont supposées utiliser une approche plus sophistiquée que l'approche de base. Une combinaison des trois méthodes [approche de base, approche standard et approche avancée] est même possible en fonction des activités, sous certaines conditions. »
L'une des nouveautés du dispositif Bâle II en matière de risques opérationnels est donc d'inciter les institutions bancaires à améliorer la gestion de leurs risques opérationnels, cette dernière étant encadrée par des exigences organisationnelles spécifiques à chacune des trois approches : plus l'organisation de la banque est complexe et sophistiquée, à travers des systèmes et des pratiques plus sensibles aux risques, plus l'approche proposée par le régulateur permet d'espérer une réduction du capital réglementaire.
L’ensemble du dispositif Bâle II a été conçu pour inciter à évoluer progressivement vers la méthode avancée, celle-ci étant en principe moins consommatrice en fonds propres réglementaires. Cette économie de fonds propres trouve sa contrepartie dans la mise en œuvre d’une organisation spécifique visant à un meilleur contrôle des risques opérationnels et, en définitive, à la réduction des pertes. C’est probablement la raison pour laquelle le régulateur a lui-même défini un code de saines pratiques à utiliser par les banques et leurs superviseurs.
Partant du principe fixé par le régulateur selon lequel un risque est correctement maîtrisé s’il est identifié, mesuré, évalué et géré, les trois approches ont pour objet de quantifier le risque opérationnel avec une sensibilité variable et donc, pour le couple superviseur / banquier, de contribuer à une meilleure surveillance prudentielle de ce dernier. Parallèlement à ces outils de mesure, le régulateur a développé dix principes de bonnes pratiques nécessaires à la maîtrise des risques opérationnels, rappelant par là l’importance tant de l’implication de l’organe exécutif dans la mise en place d’un tel système, que de l’identification des risques opérationnels, notamment au travers d’une cartographie de ces derniers.
La mise en œuvre de la méthode de base ne requiert aucune exigence organisationnelle particulière.
Si les structures des deux autres approches (standard et AMA) sont assez différentes, en raison principalement de la présence ou non d’une entité destinée spécifiquement à la gestion des risques opérationnels, leurs modalités de mise en œuvre devraient théoriquement être assez proches dans la mesure où, quelle que soit l’approche, ces modalités s’appuient d’une part sur un modèle de traitement, d’autre part sur des fonctionnalités relativement standardisées pour l’ensemble des banques :
Le modèle de traitement des risques opérationnels comporte quatre sous-processus clés nécessaires à l’élaboration d’un système de gestion adéquat :
L’identification des risques opérationnels requiert de la banque qu’elle définisse quels sont les facteurs inhérents aux risques opérationnels et leurs dimensions multiples (codification, aspect interne / externe, fréquence, appartenance, gravité, type de perte, activité(s) concernée(s), processus / fonctions concernées, données et systèmes impliqués, etc.). La mise en œuvre de ce premier sous-processus d’identification, dans le cadre du dispositif Bâle II, se heurte tout d’abord au problème d’une définition interne des risques opérationnels qui soit cohérente et compatible avec celle retenue par le dispositif lui-même, et ensuite à celui de leur identification : en effet, si les pertes opérationnelles, qui matérialisent l’occurrence des risques opérationnels, étaient jusqu’à présent identifiées et contrôlées par les départements de contrôle interne ou d’audit interne, elles deviennent dans le nouveau dispositif la responsabilité des responsables opérationnels dans tous les secteurs de la banque. La mise en œuvre de ce premier sous-processus d’identification risque d’être influencée par le contexte dans lequel fonctionne la banque (« principles based » versus « rules based »), d’autant que certains vont jusqu’à identifier le risque opérationnel comme tout risque financier autre que risque de crédit ou risque de marché. Deuxième difficulté : une perte étant intrinsèquement mesurée en faisant usage de règles comptables, en raison de son impact sur la situation financière de la banque, l’application de ces règles comptables peut donner lieu à des interprétations divergentes. Particulièrement difficile s’avère l’évaluation de certains impacts (perte de marge brute, pertes de clientèle, par exemple).
Jusqu’à présent, pour évaluer les montants des risques, les experts en matière de gestion des risques ont principalement développé leur savoir-faire dans le domaine des risques de crédit et des risques de marché, en mettant l’accent sur l’application de méthodes quantitatives et statistiques de modélisation et de simulation. Il était donc naturel que ces mêmes experts, tant au sein des banques que chez les autorités de contrôle, aient eu tendance à appliquer ces techniques éprouvées pour l’évaluation des risques opérationnels. Ainsi pourrait s’expliquer en partie la présence dans l’approche AMA de critères comparables à ceux de l’approche IRB utilisée pour les risques de crédit. Plus fondamentalement, l’application de méthodes statistiques de modélisation pour l’évaluation des risques opérationnels a fait l’objet de sévères critiques, notamment dans le monde académique. Ainsi a-t-on fait valoir que certaines caractéristiques des données des pertes opérationnelles (distributions atypiques des montants de pertes extrêmes, événements de pertes irréguliers, fréquence et sévérité des pertes non stationnaires, existence ou non de pertes répétitives) n’étaient pas cohérentes avec les postulats de modélisation. À cette première objection s’ajouterait celle d’un manque certain de données, et surtout de données cohérentes. Enfin nombreux sont ceux à souligner les difficultés à modéliser les événements à fréquence faible et à fort impact : trois types de modèles sont préconisés dans le cadre de l’approche AMA (méthode Internal Measurement Approach (IMA), méthode Loss Distribution Approach (LDA), méthode Scorecard). Certains considèrent que l’IMA a été conçue comme une version simplifiée, praticable et standardisée d’une approche actuarielle de type LDA, plus complète et plus satisfaisante, mais plus compliquée à mettre en œuvre. Ce serait sous la pression de certaines banques, notamment anglo-saxonnes, de l’IIF que la méthode Scorecard aurait été intégrée au dispositif Bâle II. C’est en raison de ces critiques que se sont développées d’autres méthodes plus dynamiques visant à gérer les risques opérationnels à travers un contrôle plus global des processus dans lesquels ces risques sont potentiellement présents. Cela implique une simulation du fonctionnement de toute la chaîne des processus, basée à la fois sur des scénarios réels et une réalité virtuelle, permettant théoriquement d’anticiper tous les éléments relatifs à un processus spécifique, mais également toutes les implications et interrelations. Cette méthode des scénarios est de plus en plus utilisée (¾ des banques sondées dans l’enquête PRMIA 2006 contre 50 % dans la même enquête 2005) : elle part du principe que les risques opérationnels associés à un processus ne peuvent être évalués séparément de l’organisation dans laquelle ce processus fonctionne ; c’est dans l’interaction d’un processus avec son environnement que se trouvent les éléments clés d’appréciation des risques opérationnels. Pour bien identifier les corrélations entre les processus et les événements de pertes, la difficulté est de bien isoler ces processus les uns des autres afin d’évaluer correctement leur part de contribution dans une perte en particulier. Cela étant, cette méthode comporte encore des domaines d’incertitude, par exemple le choix des hypothèses sous-jacentes aux scénarios majeurs (hypothèses de place), l’évaluation de certains scénarios extrêmes (grippe aviaire, par exemple), ou encore les traitements concernant les assurances.
Le suivi des risques opérationnels au moyen d’indicateurs adéquats (indicateurs d’alerte, indicateurs de risque avéré et indicateurs de pertes) est le troisième processus-clé d’un système de gestion de cette catégorie de risques. À ce stade se pose le problème de la consolidation des indicateurs, que l’on peut aborder au moyen de deux approches : bottom-up ou top-down. Dans l’approche bottom-up, les indicateurs clés des risques opérationnels sont définis et mesurés aux niveaux inférieurs, là où l’appréciation individuelle des managers exercera un levier maximum sur le suivi des risques opérationnels, pour être ensuite consolidés progressivement jusqu’à un niveau central. Dans l’approche top-down, c’est en fonction de la vision stratégique globale, de la rentabilité globale des opérations que l’allocation de capital réglementaire aux différentes activités sera décidée par les organes exécutifs en fonction de leurs risques opérationnels. Dans ce contexte, les décisions prises aux niveaux supérieurs de la hiérarchie seront répercutées et traduites en plans d’actions suivis et maîtrisés par les managers au moyen d’indicateurs adéquats.
La maîtrise et l’atténuation du risque constituent probablement le sous-processus le plus complexe de cet ensemble, car de lui va dépendre la capacité de la banque à se doter de moyens de prévenir les risques en identifiant les leviers d’action adéquats pour anticiper certains événements ou minimiser leur impact en cas de survenance. Ce sous-processus est particulièrement complexe à gérer car il s’appuie simultanément sur deux fonctions qui interagissent l’une sur l’autre :
Les deux principales fonctionnalités applicatives à mettre en œuvre pour maitriser un système de gestion des risques opérationnels sont d’une part la détermination du profil des risques opérationnels de la banque, d’autre part la mise en place d’un dispositif de collecte d’événements de risque.
Cela étant, pour assurer la couverture de leurs risques opérationnels, les banques font habituellement appel à des modèles d’allocation, les deux approches les plus utilisées étant l’approche bottom-up et l’approche top-down, ou encore une combinaison des deux. Le principe de l’approche bottom-up est de calculer le besoin en capital réglementaire au niveau le plus fin, par exemple au niveau d’une catégorie d’opérations, et de consolider ensuite ces besoins à des niveaux de plus en plus centralisés jusqu’à l’ensemble de la ligne métier à laquelle seront alloués les fonds propres correspondants. À l’inverse, le principe de l’approche top-down consiste à désagréger une information mesurée sur la totalité des risques opérationnels de la banque et d’allouer ensuite ces fonds propres à des niveaux de plus en plus décentralisés.
Malgré la simplicité de ces deux enjeux (modèle de processement et fonctionnalités d’application), il s’avère que, dans la pratique, la mise en œuvre des différentes approches a soulevé et soulève encore de nombreuses divergences dont la complexité est progressivement apparue à l’occasion des nombreuses missions d’enquête menées tant en France par la Commission Bancaire qu’à l’étranger par les autorités ou organismes compétents.
« Par ailleurs, l’allocation de fonds propres au titre des seuls risques opérationnels demeure rare. Les grands groupes bancaires ayant opté majoritairement pour une approche AMA envisagent un calcul de fonds propres pour l’ensemble du groupe bancaire et une allocation de ces derniers aux différentes entités selon une clé d’allocation et un processus comme celui décrit ci-dessous.
Rares sont les groupes qui envisagent de calculer des exigences au niveau d’une ou de plusieurs de leurs filiales, bien que les principes édictés par le Comité de Bâle relatifs à la reconnaissance transfrontière d’une approche AMA imposent un tel calcul pour les filiales significatives d’un groupe. »
« L’utilisation de données historiques internes relève en général d’une approche de type top-down, où les risques opérationnels sont d’abord identifiés et mesurés sur une base consolidée à partir de leurs pertes potentielles, et où les fonds propres sont ensuite alloués aux différentes lignes de métier. La sensibilisation croissante à cette modélisation statistique des risques opérationnels s’est heurtée pendant un temps à l’insuffisance des historiques de données internes et à des problèmes pratiques, en particulier relatifs au niveau à partir duquel toute perte doit être collectée et à la façon dont celle-ci doit être capturée aux fins d’assurer une remontée correcte des données recherchées (collecte automatique ou déclarative) et une distribution crédible des pertes. Cependant, des progrès importants ont été réalisés dans ce domaine, notamment en raison de règles de collecte et de mesure qui s’harmonisent progressivement entre banques, et aussi du fait que le recours à des données externes a été facilité grâce à la maturité des bases consortiales (ORX devenant la référence). D’où une méfiance certaine à l’égard de la seule utilisation de ces données historiques qui justifie le recours à des données externes. »
« L’utilisation de données externes soulève également des interrogations concernant la correction nécessaire de biais statistiques et l’adaptation des données externes à la situation interne de la banque (problèmes de scaling). »
« D’autres banques construisent leur modèle de mesure en privilégiant davantage des données prospectives, de type analyses de scénarios et/ou indicateurs de risque. Dans ce cas, l’approche se veut bottom-up, les risques étant cartographiés au niveau de chaque ligne de métier à partir des causes, puis mesurés sur la base de fréquences et de sévérités de pertes estimées par les experts de chaque métier et/ou d’indicateurs de performance, de contrôle et de risque. Bien que les analyses de scénarios soient considérées comme un élément important de la diffusion d’une culture du risque opérationnel, du fait qu’elles s’appuient sur l’expertise des gestionnaires au sein des métiers, elles nécessitent en général de sérieuses précautions avant d’être totalement opérationnelles : en effet, ces analyses doivent être suffisamment structurées et cohérentes pour que les quantifications subjectives des risques opérationnels au niveau des métiers puissent alimenter correctement le modèle de calcul des fonds propres au niveau consolidé. Aussi certaines banques réservent-elles ce type d’analyse aux seuls événements à faible probabilité et à forte sinistralité. »
« D’autres banques utilisent ou s’orientent vers une méthode de scorecard (indicateurs de risque ou de performance, fondés en partie sur l’utilisation de critères qualitatifs) permettant notamment d’effectuer des allocations de fonds propres réglementaires entre lignes de métiers ou entre implantations géographiques en fonction de leur capacité à maîtriser les risques opérationnels. Outre son aspect plus synthétique, cette méthode apporte un double avantage : elle introduit d’abord une dimension prospective qui s’inscrit dans une gestion active de prévention des risques opérationnels ; elle facilite ensuite le reporting aux organes exécutifs en fournissant, au moyen de tableaux de bord des performances locales, un état de progrès par rapport à la stratégie définie par ces organes pour assurer la maîtrise des risques opérationnels. En pratique, l’identification des indicateurs de risque s’effectue à partir des risques identifiés lors de la cartographie et par rapport à des indicateurs existants (indicateurs de qualité, de performance…). Sont ensuite sélectionnés des indicateurs clés de risques (KRI[10]) susceptibles de faciliter la prise de décision. Parmi les difficultés rencontrées dans la mise en place de cette méthode figure notamment l’interprétation qu’il convient de donner aux indicateurs (par exemple ceux liés aux ressources humaines), la définition de niveaux d’alertes cohérents avec la politique générale de gestion des risques opérationnels ainsi que les modalités d’agrégation des indicateurs. »
« De l’avis du régulateur lui-même, les banques [ont la volonté] d’adopter une approche plus pragmatique en termes de risque opérationnel en rééquilibrant le dispositif vers la gestion des risques plutôt que vers leur seule mesure. L’utilisation de données prospectives suppose une prise en compte des changements intervenus ou à venir dans la gestion des risques opérationnels et/ou dans les activités des établissements et donc une forte implication des gestionnaires de risque au niveau des métiers. Mais si l’utilisation de facteurs qualitatifs de type scorecards bénéficie d’une certaine expérience, notamment aux fins de l’allocation des fonds propres entre les différentes entités d’un groupe, la traduction quantitative de ces facteurs demeure problématique et n’apparaît pas véritablement stabilisée. Cette traduction quantitative est d’autant plus délicate lorsque les analyses de scénarios et les appréciations à dire d’experts ne s’inscrivent pas dans une démarche bien structurée et homogène au sein du groupe. Il est donc d’autant plus nécessaire que les établissements développent des questionnaires précis adressés aux experts des métiers ainsi que des indicateurs de risque pertinents et observables sur une base régulière, susceptibles de limiter le caractère subjectif voire parfois politique du processus de quantification[11]. »
Enfin la mise en œuvre d’un dispositif efficace de mesure et de gestion du risque opérationnel, quelles que soient les modalités d’analyse des données, requiert un système d’information adéquat. C’est là probablement un des domaines où les banques ont encore d’importants progrès à accomplir, ce qui n’est probablement pas étranger au fait que la direction des systèmes d’information n’est pas souvent représentée au sein des banques dans les comités de gestion des risques opérationnels. L’adaptation des systèmes d’information aux exigences spécifiques du processement des risques opérationnels a donc amené les banques à faire le choix entre le lancement d’un projet entièrement nouveau ou à la réalisation d’extensions destinées à collecter les données nécessaires. Dans le premier cas, il s’est agi de mettre en œuvre une procédure entièrement nouvelle de collecte systématique des pertes et, à cet effet, de conduire des missions de sensibilisation à tous les échelons de la banque. Dans le second cas, il s’est plutôt agi de reprendre et de retraiter au niveau des métiers des historiques de pertes existants.
Dans un environnement aussi complexe, il est clair que la définition du véritable profil de risque d’un grand groupe bancaire et la mise en place d’une politique efficace de réduction des pertes opérationnelles dans chaque entité relève d’un projet global, nécessitant un déploiement à grande échelle, et partant, une réelle gestion du changement.
Les enjeux de conduite du changement associés à Bâle II concernent notamment la diffusion d’une nouvelle culture de vigilance à propos des risques opérationnels, et la pérennisation de ce système.
Le premier enjeu consiste à diffuser une culture de vigilance à l’égard de ces risques dans chaque business unit de la banque. À ce titre, on peut parler de véritable acculturation des collaborateurs présente dans toutes les modalités de la mise en œuvre (cartographie des risques opérationnels, dispositif de collecte des incidents), le principal attribut de cet enjeu étant l’implication de chaque collaborateur de la banque.
Le second enjeu est d’éviter que le système de gestion des risques opérationnels ne devienne figé, et donc de faire en sorte qu’il puisse évoluer sous l’effet des actions correctives, des risques qui disparaissent, et des nouveaux risques qui apparaissent.
Globalement, l’enjeu du processement régulatoire des risques opérationnels n’est pas d’obtenir une certification du superviseur pour que la banque puisse utiliser telle ou telle approche proposée par le régulateur : il est de favoriser une amélioration durable de la maîtrise des risques opérationnels par la banque, en responsabilisant chacune des parties prenantes (régulateur, superviseur, collaborateur de la banque). C’est bien là où le dispositif acquiert un degré supplémentaire de complexité puisqu’en définitive, la réussite de la mise en œuvre du dispositif est tributaire non seulement des bonnes pratiques de la banque mais aussi de la flexibilité de la surveillance prudentielle exercée par le superviseur et de l’adaptabilité introduite dans le dispositif par le régulateur.
C’est principalement dans l’application de l’approche AMA que va s’exercer l’influence des superviseurs.
Pour ce qui concerne la France, la Commission Bancaire a opté pour une position « flexible » au cas par cas, c'est-à-dire banque par banque.
Pour cela, la Commission Bancaire prendra d’abord en compte la manière dont chaque banque aura proportionné son approche AMA avec son profil de risques opérationnels.
Cette autorité ne privilégiera aucune méthode plutôt qu’une autre, se réservant seulement d’apprécier la pertinence d’ensemble des méthodologies retenues par rapport au profil de risque.
La Commission Bancaire se voudrait étrangère à toute approche trop normative et figée, incompatible avec la dimension évolutive des techniques et méthodologies développées par les banques dans le domaine des risques opérationnels.
Pour ses démarches de validation, la Commission Bancaire exprime d’abord un souci de cohérence avec la surveillance prudentielle exercée par d’autres autorités de contrôle, en reprenant à son compte les principes de home-host supervision et les principes d’une approche hybride entre filiales significatives et autres filiales, tels qu’ils sont proposés dans le dispositif Bâle II. Elle précise ensuite les grandes lignes du contenu de sa démarche, en parfaite conformité avec le contenu du dispositif :
Finalement, c’est plus par sa logique d’intervention que par sa flexibilité que la surveillance prudentielle va subir un changement significatif : la vérification exhaustive des critères d’éligibilité, tant qualitatifs que quantitatifs, à l’AMA, ne s’inscrit donc pas dans une simple logique d’appréciation de la conformité réglementaire d’un modèle. Elle repose surtout sur une évaluation de la capacité des établissements à identifier, analyser, maîtriser et réduire (tant la fréquence que la sévérité des pertes) leurs risques opérationnels[11].
