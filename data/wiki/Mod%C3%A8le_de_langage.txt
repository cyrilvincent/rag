Ne doit pas être confondu avec Grand modèle de langage qui est un cas particulier de modèle de langage
modifier - modifier le code - modifier Wikidata
En traitement automatique des langues, un modèle de langage, modèle de langue ou modèle linguistique est un modèle statistique de la distribution de symboles distincts (lettres, phonèmes, mots) dans une langue naturelle. Un modèle de langage vise fondamentalement à prédire le mot suivant dans une séquence de mots.
Un modèle de langage n-gramme est un modèle de langage qui modélise des séquences de mots comme un processus de Markov. Il utilise l'hypothèse simplificatrice selon laquelle la probabilité du mot suivant dans une séquence ne dépend que d'une fenêtre de taille fixe de mots précédents. Un modèle digramme considère un mot précédent, un modèle trigramme en considère deux, et en général, un modèle n-gramme considère n-1 mots du contexte précédent.
Les modèles n-grammes ne sont plus couramment utilisés dans la recherche et les applications du traitement du langage naturel, car ils ont été supplantés par les méthodes d'apprentissage en profondeur, plus récemment  les grands modèles de langages.
Un modèle de langage de grande taille (large language model ou LLM en anglais) s'appuie sur de vastes corpus de textes de diverses sources, tels que livres, articles de presse, pages Web, forums, réseaux sociaux, etc. pour prédire, à partir d'un mot donné, les mots et les phrases suivants dans un texte.
Les LLM sont utilisés pour une variété de tâches, telles que la génération de texte, la traduction automatique, la classification de texte et la réponse aux questions. Parmi les exemples de LLM les plus connus, on peut citer GPT-3 et GPT-4 de OpenAI et BERT de Google.
Outre les grands modèles de langage, les petits modèles de langage (SLM ou small langage model) nécessitent moins de données et de puissance de calcul pour fonctionner. Cette caractéristique les rend plus pratiques pour une utilisation dans des appareils avec des ressources limitées ou pour des applications nécessitant une réponse rapide.
Les LLM sont utilisés pour une variété de tâches, telles que la génération de texte, la traduction automatique, la classification de texte et la réponse aux questions.
Plus précisément, Solaiman, I. et al. rapportent des usages dits bénéfiques dans une grande variété de domaines. Ils ressortent :
Bender et al. 2021 remet en cause la pertinence des énormes modèles de langage pré-entraînés. Ils extraient 3 catégories de problèmes : le coût environnemental de l’entraînement et de l’utilisation de modèles de grande taille, les problèmes issus des jeux d'entraînements tirés d’Internet, et les problèmes introduits par la “cohérence apparente” de ces modèles. Parmi les problèmes issus des jeux d’entraînements, se trouvent les biais que les modèles statistiques encodent souvent, le manque de supervision et de responsabilité dans le traitement des jeux de données, ainsi que l’idée que les mœurs sociales changent, mais que les modèles de langages sont incapables de se modifier sans réentraînement actif avec de nouvelles données.
Bender et al. critiquent aussi le fait qu’il n’y a pas de pensée derrière la communication des LLM ; parce que c’est un modèle statistique, la sortie ne serait donc qu’une imitation d’une communication réelle.
La question des droits d'auteur liés à l'utilisation de données textuelles protégées pour l'entraînement de modèles de langage pré-entraînés est une préoccupation majeure, comme l'explique Christopher T. Zirpoli 2023. La controverse réside principalement dans la distinction entre l'utilisation transformative, qui peut être considérée comme une utilisation équitable, et la reproduction de travaux protégés par le droit d'auteur. Bien que ces modèles ne reproduisent pas directement le contenu source, ils génèrent des sorties basées sur ces données, pouvant éventuellement reproduire des segments de textes protégés. La législation actuelle ne couvre pas explicitement ce domaine, laissant une zone d'incertitude juridique. Cela souligne la nécessité d'une clarification légale et d'un débat éthique autour de la propriété intellectuelle dans le contexte de l'intelligence artificielle générative.
L'article “Challenges of Context and Time in Reinforcement Learning : Introducing Space Fortress as a Benchmark” par Akshat Agarwal et al. (2018) aborde des problématiques essentielles dans le domaine de l'apprentissage par renforcement. Les auteurs mettent l'accent sur la complexité d'intégrer le contexte et le facteur temporel dans les algorithmes. Ces derniers font face à des défis majeurs lorsqu'il s'agit de prendre des décisions optimales en temps réel, en raison des incertitudes et de la complexité inhérente aux situations réelles. La problématique est d'autant plus pressante qu'une mauvaise interprétation du contexte ou un manque d'adaptation à l'évolution rapide des environnements peut conduire à des résultats imprévus et loin d'être optimaux.
Tel que mentionné par Bender et al., les modèles de langage sont directement impactés par la qualité de leurs données d'entraînement. Plusieurs biais ont donc pu être démontrés dans des versions de modèles. On peut nommer comme exemple la priorisation des productions anglophones, pour les modèles entraînés en anglais. Les modèles peuvent aussi être affectés par les biais humains comme les stéréotypes de genre, religion, race, etc.
De plus, il est possible d’utiliser des modèles open-source, même si souvent moins perfectionnés que ceux de l’industrie privée, et d’en accentuer une idéologie désirée en lui fournissant des données la supportant. Fraioli croit que cette facilité à automatiser la génération de textes pourrait augmenter la quantité de contenu de propagande accessible sur le web.
La présence d’opinion dans les modèles de langage peut influencer les opinions de ses utilisateurs, selon une étude de Jakesch et al. Solaiman, I. et al. soutiennent qu’il est alors primordial d’analyser les modèles et de les tester pour ces biais. Selon eux, des méthodes standardisées au travers de l'industrie sont nécessaires.
D’un point de vue politique, plusieurs pays prévoient une influence des modèles de langage sur leurs affaires internes. Par exemple, avec la montée de la désinformation, Fraioli croit qu’il faut surveiller les campagnes électorales prochaines et y mesurer l’impact des modèles.
De plus, la dynamique de la recherche en IA est largement dominée par quelques grandes entreprises technologiques, souvent américaines, qui disposent des ressources nécessaires pour développer et déployer des jeux de données volumineux, des modèles et des clusters de calcul. Les relations internationales se sont donc vues impactées par la montée en popularité des modèles de langage, car leur développement a créé une dynamique de nationalisme de l’IA avec une course à la scalabilité ascendante, principalement entre les États-Unis et la Chine. Fraioli ressort le contrôle par les États-Unis des exportations de semi-conducteurs, souvent utilisés en AI, vers la Chine comme un exemple de ces nouvelles mesures. Il estime que ce serait pour préserver les avancées américaines dans le domaine, ou pour limiter l’influence Chinoise à l’international. Par contre, il soutient que ça pourrait encourager la Chine à investir dans ses propres infrastructures.
Finalement, tous les pays n’ont pas la même approche quant à la régularisation des modèles de langage. L’Union Européenne, par exemple, prévoit être plus sévère et cela pourrait ralentir les développements de l’IA à long terme en Europe.
