modifier - modifier le code - modifier Wikidata
Un grand modèle de langage, grand modèle linguistique, grand modèle de langue,, modèle de langage de grande taille ou encore modèle massif de langage (abrégé LLM de l'anglais large language model) est un modèle de langage possédant un grand nombre de paramètres (généralement plus d'un milliard).
Ce sont des réseaux de neurones profonds entraînés sur de grandes quantités de texte non étiqueté utilisant l'apprentissage auto-supervisé ou l'apprentissage semi-supervisé. Les LLM sont apparus vers 2018 et ont été utilisés pour la mise en œuvre d'agents conversationnels.
Ils excellent dans un large éventail de tâches. Au lieu d'être entraînés pour une tâche spécifique comme l'analyse des sentiments, la reconnaissance d'entités nommées ou le raisonnement mathématique, ils sont entraînés à prédire une suite probable pour une entrée donnée. La qualité du contenu généré semble augmenter régulièrement avec le nombre de paramètres, la taille et la qualité des données d'entraînement, ainsi que la quantité de calculs utilisée pour entraîner le modèle.
Les modèles de langage possédant un grand nombre de paramètres s'avèrent capables de capturer une grande partie de la syntaxe et de la sémantique du langage humain. Cela permet de retraduire une connaissance générale sur le monde conséquente, avec « mémorisation » d'une grande quantité de faits lors de l'entraînement.
Avant le succès des grands modèles de langage, l'attention des chercheurs en traitement automatique des langues était principalement focalisée sur l'apprentissage supervisé de modèles spécialisés pour des tâches spécifiques.
Un modèle de langage reçoit typiquement en entrée des données séquentielles de longueur variable. Pendant longtemps, l'architecture utilisée préférentiellement pour ce genre de données était celle dite de réseaux de neurones récurrents. Cette architecture présentait comme inconvénient majeur de mal se prêter à la parallélisation des calculs nécessaires à l'entrainement.
En 2017, un article très influent suggère qu'une architecture non-récurrente, le transformeur, basée sur un mécanisme d'attention, peut avec succès traiter des données séquentielles tout en étant parallélisable lors de l'entrainement.
C'est l'avènement de cette architecture, et surtout les gains en performance qu'elle procure, qui permettent aux chercheurs d'augmenter considérablement le nombre de paramètres de leurs modèles, d'où le qualificatif « grand » les concernant. La plupart des grands modèles de langage utilisent donc cette architecture, même si la recherche se poursuit pour trouver des architectures encore plus performantes,[10],[11],[12].
Les LLM sont des fonctions mathématiques dont l'entrée et la sortie sont des listes de nombres. Pour que ceux-ci fonctionnent avec des mots une conversion est nécessaire.
Cette conversion est ce qu'on appelle l'analyse lexicale. L'analyseur lexical est une fonction bijective qui établit une correspondance entre des textes et des listes de nombres entiers. Il est généralement d'abord adapté à l'ensemble complet des données d'entraînement, puis gelé, avant que le modèle de langage ne soit entraîné. Un choix courant est le codage par paires d'octets.
Une autre fonction des analyseurs lexicaux est la compression de texte, qui épargne du temps de calcul. Des mots ou expressions courants tels que « où est » peuvent être encodés dans une seule unité lexicale (ou jeton, token en anglais), au lieu d'être encodés par 6 caractères. La série OpenAI GPT utilise un analyseur lexical où une unité lexicale correspond à environ 4 caractères, soit environ 0,75 mots dans un texte anglais courant[13]. Un texte anglais peu courant est moins prévisible, donc moins compressible, nécessitant ainsi plus de jetons pour être encodé.
Un analyseur lexical convertit une suite de caractères en un nombre entier dans la plage 
, où 
 est appelé la taille de vocabulaire.
Certains sont capables de gérer des textes arbitraires en opérant généralement directement sur Unicode, mais d'autres non. Lorsqu'il rencontre du texte non encodable, un analyseur lexical génère une unité lexicale spéciale (par exemple 0) qui représente un « texte inconnu ». Ceci est souvent écrit comme [UNK], comme dans l'article du modèle BERT.
Une autre unité lexicale spéciale couramment utilisée est [PAD] (souvent 1), pour « padding ». Ceci est utilisé car les LLM sont généralement utilisés sur différents lots de texte à la fois, et ces textes ne sont pas codés à la même longueur. Étant donné que les LLM exigent généralement que l'entrée soit un tableau de taille fixe, les textes les plus courts doivent être complétés.
La sortie d'un LLM est un vecteur 
où 
 est la taille du lexique produit lors de l'analyse lexicale. Le vecteur 
 est ensuite passé par une fonction softmax pour obtenir 
 ;
Puisque le vecteur 
 a 
 entrées, toutes non négatives, et dont la somme est égale à 1, on peut l'interpréter comme une distribution de probabilité sur le vocabulaire du LLM (indexé par 
).
La plupart des LLM commencent leur apprentissage par un pré-entraînement génératif, c'est-à-dire qu'à partir d'un ensemble de données d'entraînement de jetons de texte, le modèle prédit les jetons dans l'ensemble de données. Il existe deux styles généraux de pré-entrainement pour la génération[14] :
Les LLM peuvent être entraînés sur des tâches auxiliaires qui testent leur compréhension de la distribution des données, telles que la prédiction de la phrase suivante (NSP), dans laquelle des paires de phrases sont présentées et le modèle doit prédire si elles apparaissent consécutivement dans le corpus d'entraînement.
Habituellement, les LLM sont entraînés pour minimiser une fonction de perte spécifique : la log-vraisemblance négative moyenne par jeton (également appelée perte d'entropie croisée).[réf. nécessaire] Par exemple. si étant donné « j'aime manger », un modèle autorégressif prédit une distribution de probabilité 
J'aime manger
 alors la perte de vraisemblance logarithmique négative sur ce jeton est 
log
des
J'aime manger
 .
Pendant l'entraînement, la perte de régularisation est également utilisée pour stabiliser l'entraînement. Cependant, celle-ci n'est que peu utilisée pendant les tests et l'évaluation. Il y existe beaucoup d'autres critères d'évaluation que la simple vraisemblance logarithmique négative, comme le décrit la section ci-dessous.
Les premiers LLM ont été entraînés sur des corpus contenant des milliards de mots.
GPT-1, le premier modèle de la série de transformeurs génératifs pré-entraînés d'OpenAI, est entrainé en 2018 sur BookCorpus, composé de 985 millions de mots[15]. La même année, le modèle BERT de Google est entraîné sur une combinaison de BookCorpus et de Wikipedia anglais, totalisant 3,3 milliards de mots. Depuis, les corpus d'entraînement pour les LLM ont augmenté de plusieurs ordres de grandeur, comme le modèle LLaMA 3 de META sorti en avril 2024, qui utilise 15 billions de jetons[16].
L'entraînement des LLM est coûteux en termes de calcul. Une étude de 2020 estime le coût de l'entraînement d'un modèle de 1,5 milliard de paramètres (2 ordres de grandeur inférieurs à l'état de l'art à l'époque) à 1,6 million de dollars. Depuis, les progrès logiciels et matériels ont considérablement réduit ces coûts, comme relaté dans un article de 2023 qui état d'un coût de 72 300 heures A100-GPU pour entraîner un modèle de 12 milliards de paramètres. La taille des modèles a cependant aussi augmenté[réf. souhaitée].
Pour un LLM utilisant un transformeur, le coût d'entraînement du modèle est beaucoup plus élevé que son coût d'inférence. L'entraînement sur un jeton coûte 6 FLOPS par paramètre, alors que l'inférence sur un jeton coûte de 1 à 2 FLOPS par paramètre[17].
Le réglage fin (fine-tuning), ou ajustement, est la pratique consistant à modifier un modèle de langage pré-entraîné existant en l'entraînant (souvent de manière supervisée) sur une tâche spécifique (par exemple, l'analyse des sentiments, la reconnaissance d'entités nommées ou le marquage d'une partie du discours). C'est une forme d'apprentissage par transfert qui permet de spécialiser le LLM.
Le réglage fin peut se faire de différentes façons, notamment[18] :
Le réglage par instructions (instruction fine-tuning en anglais) est une forme d'apprentissage supervisé visant à ce que le LLM réponde d'une certaine façon, par exemple en adoptant le rôle d'assistant. Étant donné un prompt en entrée, un modèle de langage pré-entraîné générera une complétion qui correspond à la distribution du texte sur laquelle il a été entraîné. En réponse au prompt « Écrire un essai sur les principaux thèmes de Hamlet. », un modèle de langage pré-entraîné pourrait produire : « Une pénalité de retard de 10 % par jour sera appliquée aux soumissions reçues après le 17 mars ». Dans le réglage par instructions, le modèle de langage est entraîné sur de nombreux exemples de tâches formulées sous forme d'instructions en langage naturel, ainsi que des réponses appropriées. Les instructions et les réponses correspondantes attendues peuvent être soit écrites par des humains, soit générées automatiquement, par exemple avec la méthode self-instruct[19].
Les grands modèles de langage suivent souvent une phase supplémentaire de réglage fin exploitant l'apprentissage par renforcement, afin de les rendre utiles et inoffensifs et de réduire les hallucinations.
Une méthode populaire est celle de l'apprentissage par renforcement à partir de rétroaction humaine (reinforcement learning from human feedback, ou RLHF en anglais). Avec cette méthode, différentes réponses sont d'abord générées par le modèle, avant d'être annotées par des humains en fonction de leur vraisemblance. Avec ces notations, un « modèle de préférences » est entraîné à prédire quelles réponses satisferaient les annotateurs humain. Les réponses fausses, agressives ou inutiles sont ainsi le plus souvent évaluées comme étant mauvaises. D'autres modèles peuvent ensuite être entraînés par apprentissage par renforcement à satisfaire ce modèle de préférences[20].
La méthode de l'IA constitutionnelle, conçue par l'entreprise Anthropic, comporte une étape d'apprentissage par renforcement, en plus de celle des réglages par instructions. Le but est là aussi de générer un modèle de préférences permettant ensuite d'entraîner d'autres modèles. La différence est que les données servant à entraîner ce modèle de préférences ne sont pas annotées manuellement par des humains. Ces données sont générées automatiquement par un modèle en évaluant quelles réponses satisfont le mieux une constitution donnée, qui est une liste de principes de conduite[21].
Les LLM sont pré-entraînés sur de grands ensembles de données textuelles comme Common Crawl[22], The Pile[23], MassiveText[24], Wikipedia ou GitHub. Ces ensembles de données contiennent jusqu'à 10 000 milliards de mots.
Le stock de données linguistiques de haute qualité se situe entre 4,6 et 17 milliards de mots, soit un ordre de grandeur similaire à celui des plus grands ensembles de données textuelles disponibles[25].
En général, les performances d'un réseau de neurones augmentent régulièrement avec[26],[27] :
Les relations empiriquement observées entre les performances d'un réseau de neurones et chacun de ces trois paramètres sont tellement précises qu'elles ont été nommées « lois d'échelle » (scaling laws)[26],[27]. Par exemple, une loi de mise à l'échelle particulière (Chinchilla scaling) pour le LLM Chinchilla (en) entraîné de manière autorégressive (c'est-à-dire avec prédiction du mot suivant étant donné un segment de texte) pour une époque, avec un calendrier de taux d'apprentissage log-log, stipule que[28]:
où les variables sont :
et les paramètres statistiques sont :
Généralement les performances de grands modèles de langage sur diverses tâches peuvent être extrapolées sur la base des performances de modèles plus petits similaires. Cependant, les grands modèles subissent parfois un « déphasage discontinu » où le modèle acquiert soudainement des capacités substantielles non vues dans les modèles plus petits. Celles-ci sont connues sous le nom de « capacités émergentes » et ont fait l'objet d'études approfondies. Les chercheurs notent que de telles capacités « ne peuvent pas être prédites simplement en extrapolant les performances de modèles plus petits »[30]. Ces capacités sont découvertes plutôt que programmées ou conçues, dans certains cas seulement après le déploiement public du LLM. Des centaines de capacités émergentes ont été décrites. Les exemples incluent le raisonnement arithmétique, la passation d'examens de niveau universitaire, l'identification du sens voulu d'un mot[30], le décodage de l'alphabet phonétique international, le fait de pouvoir comprendre des mots dont l'ordre des lettres est modifié, l'identification du contenu offensant dans les paragraphes de l'hinglish (un mélange d'hindi et d'anglais) et la génération d'un équivalent anglais aux proverbes kiswahili[31].
Les LLM génèrent parfois des affirmations fausses qui ne semblent pas être justifiées par leurs données d'entraînement, on parle alors d'« hallucination »[32].
Les biais sont une préoccupation car tout modèle complexe créé par des humains peut refléter les biais des équipes qui préparent et conçoivent les LLM, et des scientifiques des données qui entraînent et mettent en œuvre les modèles[33].
Les préjugés sexistes font référence à la tendance de ces modèles à produire des résultats injustement préjugés en faveur d’un sexe plutôt qu’un autre. Ce biais provient généralement des données sur lesquelles ces modèles sont formés. Par exemple, les grands modèles linguistiques attribuent souvent des rôles et des caractéristiques basés sur les normes de genre traditionnelles ; elle pourrait associer principalement les infirmières ou les secrétaires aux femmes et les ingénieurs ou les PDG aux hommes[34].
Au-delà du genre et de l'origine, ces modèles peuvent renforcer un large éventail de stéréotypes, notamment ceux fondés sur l’âge, la nationalité, la religion ou la profession. Cela peut conduire à des résultats qui généralisent ou caricaturent injustement des groupes de personnes, parfois de manière préjudiciable ou désobligeante[35].
Les préjugés politiques font référence à la tendance des algorithmes à favoriser systématiquement certains points de vue politiques, idéologies ou résultats par rapport à d’autres. Les modèles linguistiques peuvent également présenter des préjugés politiques. Étant donné que les données de formation incluent un large éventail d'opinions et de couvertures politiques, les modèles peuvent générer des réponses qui penchent vers des idéologies ou des points de vue politiques particuliers, en fonction de la prévalence de ces points de vue dans les données[36].
Les biais linguistiques font référence au fait que les données d'apprentissage peuvent ne pas être représentatives de la population mondiale. Par exemple, pour un concept comme le « libéralisme », un modèle principalement entraîné en anglais privilégiera une interprétation anglo-américaine, centrée sur les droits de l'homme et l'égalité. Des points de vue d'autres cultures sur le libéralisme, comme « s'oppose à l'État » au Vietnam, ou « limitation du pouvoir gouvernemental » en Chine, risquent d'être sous-représentées[37].
Pour limiter ces biais, il est possible de régulièrement mettre à jour la qualité et la taille des bases de connaissances des LLM. Des processus dits « d'IA responsable » peuvent corriger une grande partie des biais. Ils le font via des techniques et des  outils ad hoc, c'est-à-dire développés (et en cours d'amélioration) pour « garantir que les systèmes d'IA peuvent respecter ces définitions, en traitant les données au préalable, en modifiant les décisions du système par la suite, ou en intégrant les définitions d'équité dans le processus de formation lui-même ». Ces techniques s'appuient notamment sur des audits par des tiers indépendants, des études scientifiques multidisciplinaires sur les biais, et des « équipes rouges » internes chargées de tester le système utilisant le LLM[38].
Un autre processus responsable est de diversifier la communauté de l'IA (pour collaborativement mieux anticiper, examiner et repérer les biais, en faisant participer les communautés discriminées et affectées par ces biais)[38]. Il est aussi nécessaire, lors des phases d'apprentissage du LLM, de diversifier les sources de données ; et dans le même temps d'accorder plus de poids aux données et conversations basées sur des faits et/ou basés sur des preuves (ex. : médecine fondée sur les faits) et sur la science (par rapport aux conversations basées sur des opinions), grâce à l'exécution d'algorithmes régulièrement contrôlés par des humains, grâce à des analyses comparées des résultats, et à la mobilisation de techniques d'explicabilité des résultats[38]. Dans certains cas des données synthétiques conçues pour être aussi peu biaisées que possibles peuvent être utilisées.
Entre 2018 et 2020, la méthode standard pour exploiter un LLM pour une tâche spécifique de traitement automatique des langues consistait à affiner le modèle avec un entraînement supplémentaire spécifique à la tâche. Il a ensuite été découvert que des LLM plus puissants tels que GPT-3 peuvent résoudre des tâches sans entraînement supplémentaire via des techniques dans lesquelles le problème à résoudre est présenté au modèle sous forme de requête (synonyme de « prompt » ou « invite »[41]), éventuellement avec quelques exemples textuels de problèmes similaires avec les réponses correspondantes attendues afin de guider le modèle[30].
L'approche qui consiste à fournir d'abord quelques exemples de réponses valides pour des requêtes similaires est appelée « requête en quelques coups » (few-shot prompt). Par exemple, une tâche d'analyse des sentiments consistant à déterminer le sentiment d'une critique de film pourrait être formulée avec la requête[30] :
Si le modèle affiche « positif », alors il a correctement résolu la tâche. Dans la requête à partir de zéro (zero-shot prompt), aucun exemple n'est fourni. Un exemple de requête à partir de zéro pour la même tâche d'analyse des sentiments serait
Il a été démontré que les performances en quelques coups de LLM permettent d'obtenir des résultats compétitifs sur des tâches de traitement automatique des langues, dépassant parfois les approches de réglage fin. Par exemple en traduction, en réponse aux questions, en complétion de texte ou en programmation[source insuffisante][42]. La création et l'optimisation de requêtes concerne l'ingénierie de prompt, aussi appelée « rédactique[43] ».
Une autre méthode parfois utilisée est celle de la chaîne de pensée. Par exemple en ajoutant dans la requête « raisonnons étape par étape ». Cette méthode peut s'avérer efficace pour les problèmes nécessitant plusieurs étapes de raisonnement, tels que les problèmes d'arithmétique[44].
La mesure la plus couramment utilisée de la performance d'un modèle de langage est sa perplexité sur un corpus de texte donné. La perplexité est une mesure de la capacité d'un modèle à prédire le contenu d'un ensemble de données ; plus la probabilité que le modèle attribue à l'ensemble de données est élevée, plus la perplexité est faible. Mathématiquement, la perplexité est définie comme l'exponentielle de la moyenne de la log-vraisemblance négative par jeton :
log
Perplexity
log
token
context for token
ici 
 est le nombre de jetons dans le corpus de texte, et le « contexte du jeton i » dépend du type spécifique de LLM utilisé. Si le LLM est autorégressif, alors le « contexte pour le jeton i » est le segment de texte apparaissant avant le jeton i. Si le LLM est masqué, alors « contexte pour le jeton i » est le segment de texte entourant le jeton i.
Étant donné que les modèles de langage peuvent suradapter à leurs données d'apprentissage, les modèles sont généralement évalués en fonction de leur perplexité sur un ensemble de tests de données invisibles. Cela présente des défis particuliers pour l'évaluation de grands modèles de langage. Au fur et à mesure qu'ils sont entraînés sur des corpus de texte de plus en plus volumineux largement extraits du Web, il devient de plus en plus probable que les données d'entraînement des modèles incluent par inadvertance des parties d'un ensemble de tests donné.
Cette notion est à l'origine du nom de la startup d'IA Perplexity AI, créée en août 2022[45].
Un grand nombre d'ensembles de données de test de validation ou de performance ont également été développés pour évaluer les capacités des modèles de langage sur des tâches en aval plus spécifiques. Les tests peuvent être conçus pour évaluer une variété de capacités, y compris les connaissances générales, le raisonnement de bon sens et la résolution de problèmes mathématiques.
Une grande catégorie d'ensembles de données d'évaluation est les ensembles de données de questions-réponses, consistant en des paires de questions et de réponses correctes, par exemple, (« Les Sharks de San Jose ont-ils remporté la Coupe Stanley? » , « Non »). Une tâche de réponse aux questions est considérée comme un « livre ouvert » si l'invite du modèle comprend un texte à partir duquel la réponse attendue peut être dérivée (par exemple, la question précédente pourrait être jointe à un texte qui comprend la phrase « Les Sharks ont atteint la coupe Stanley finales une fois, perdant contre les Penguins de Pittsburgh en 2016. »). Sinon, la tâche est considérée comme « livre fermé », et le modèle doit s'appuyer sur les connaissances retenues pendant l'entrainement. Voici quelques exemples d'ensembles de données de réponse aux questions couramment utilisés : TruthfulQA, Web Questions, TriviaQA et SQuAD.
Les ensembles de données d'évaluation peuvent également prendre la forme d'une complétion de texte, le modèle sélectionnant le mot ou la phrase la plus probable pour compléter une invite, par exemple : « Alice était amie avec Bob. Alice est allée rendre visite à son amie, ____ ».
Certains repères composites ont également été développés, qui combinent une diversité d'ensembles de données et de tâches d'évaluation différents. Les exemples incluent GLUE, SuperGLUE, MMLU, BIG-bench et HELM[46].
Auparavant, il était courant de rapporter les résultats sur une partie non conservée d'un ensemble de données d'évaluation après avoir effectué un réglage fin supervisé sur le reste. Il est maintenant plus courant d'évaluer un modèle pré-formé directement par des techniques d'incitation, bien que les chercheurs varient dans les détails de la façon dont ils formulent des invites pour des tâches particulières, en particulier en ce qui concerne le nombre d'exemples de tâches résolues qui sont associés à l'invite.
En raison du rythme rapide d'amélioration des grands modèles de langage, les repères d'évaluation ont souffert de courtes durées de vie, les modèles de pointe « saturant » rapidement les repères existants, dépassant les performances des annotateurs humains, conduisant à des efforts pour remplacer ou augmenter le repère avec tâches plus exigeantes.
Certains ensembles de données ont été construits de manière contradictoire, en se concentrant sur des problèmes particuliers sur lesquels les modèles de langage existants semblent avoir des performances inhabituellement médiocres par rapport aux humains. Un exemple est l'ensemble de données TruthfulQA, un ensemble de données de questions-réponses composé de 817 questions auxquelles les modèles de langage sont susceptibles de répondre de manière incorrecte en imitant les faussetés auxquelles ils ont été exposés à plusieurs reprises pendant l'entraînement. Par exemple, un LLM peut répondre « Non » à la question « Pouvez-vous apprendre de nouveaux tours à un vieux chien ? » en raison de son exposition à l'expression anglaise, vous ne pouvez pas apprendre de nouveaux tours à un vieux chien, même si ce n'est pas littéralement vrai.
Un autre exemple d'ensemble de données d'évaluation contradictoire est Swag et son successeur, HellaSwag, des collections de problèmes dans lesquels l'une des multiples options doit être sélectionnée pour compléter un passage de texte. Les complétions incorrectes ont été générées par échantillonnage à partir d'un modèle de langage et filtrage avec un ensemble de classificateurs. Les problèmes qui en résultent sont insignifiants pour les humains, mais au moment où les ensembles de données ont été créés, les modèles de langage de pointe étaient peu précis. Par exemple:
Nous voyons un panneau indiquant un centre de remise en forme. Nous voyons ensuite un homme parler à la caméra et assis et allongé sur un ballon d'exercice. L'homme... a) montre comment augmenter l'efficacité de l'exercice en faisant monter et descendre des balles. b) bouge tous ses bras et ses jambes et développe beaucoup de muscles. c) joue ensuite la balle et nous assistons à une démonstration de graphisme et de taille de haie. d) effectue des redressements assis tout en étant sur le ballon et en parlant.
BERT sélectionne b) comme l'achèvement le plus probable, bien que la bonne réponse soit d).
