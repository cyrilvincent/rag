Dans le domaine de l'intelligence artificielle générative, un petit modèle de langage (SLM ou small language model) est un système d'intelligence artificielle conçu pour comprendre et générer du langage humain à une échelle réduite par rapport aux grands modèles de langage.
Un petit modèle de langage est un système informatique qui utilise des techniques d'apprentissage automatique pour traiter le langage naturel mais, contrairement aux grands modèles de langage, les SLM nécessitent moins de données et de puissance de calcul pour fonctionner. Cette caractéristique les rend plus pratiques pour une utilisation dans des appareils avec des ressources limitées ou pour des applications nécessitant une réponse rapide.
Il n'existe pas de limite précise du nombre de paramètres permettant de distinguer entre les petits et les grands modèles de langage.
Les petits modèles de langage sont utilisés pour diverses fonctions, telles que la traduction automatique, la génération de texte, la complétion et la correction d'écrits, la reconnaissance vocale et la transcription, et les assistants virtuels. Ces usages alimentent dans une grande variété d'applications, allant des assistants vocaux aux outils d'aide à la rédaction. Ils sont particulièrement utiles dans les environnements à ressources limitées, comme les appareils mobiles ou les systèmes embarqués.
La taille plus petite des SLM les rend plus rapides et moins coûteux à entraîner, tout en étant suffisamment performants pour de nombreuses tâches. Les SLM sont appréciés pour leur efficacité et leur accessibilité, permettant une intégration plus large dans les produits et services.
Les avantages des SLM incluent leur coût réduit et leur empreinte carbone plus faible, car ils nécessitent moins d'énergie pour fonctionner. De plus, ils peuvent être mis à jour plus fréquemment et personnalisés pour des tâches spécifiques, offrant ainsi une flexibilité accrue.
Malgré leurs avantages, les SLM ont des limitations. Leur compréhension du langage peut être moins nuancée que celle des grands modèles, et ils peuvent avoir du mal avec des tâches complexes ou des nuances subtiles du langage. De plus, la qualité des données utilisées pour leur entraînement est cruciale pour leur performance.
Plusieurs SLM sont dérivés du grand modèle de langage BERT, fondé sur la technologie des transformeurs :
D'autres petits modèles de langage n'utlisent pas la technologie des transformeurs :
