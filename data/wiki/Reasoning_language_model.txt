Reasoning language models (RLMs) are large language models that have been further trained to solve multi-step reasoning tasks. These models perform better on logical, mathematical or programmatic tasks than traditional autoregressive LLMs, have the ability to backtrack, and employ test-time compute as an additional scaling axis beyond training examples, parameter count, and train-time compute.
o1-preview, an LLM with enhanced reasoning, was released in September 2024. The full version, o1, followed in December 2024. OpenAI also began sharing results on its successor, o3.
The development of reasoning LLMs has illustrated what Rich Sutton termed the "bitter lesson": that general methods leveraging computation often outperform those relying on specific human insights. For instance, some research groups, such as the Generative AI Research Lab (GAIR), initially explored complex techniques like tree search and reinforcement learning in attempts to replicate o1's capabilities. However, they found, as documented in their "o1 Replication Journey" papers, that knowledge distillation — training a smaller model to mimic o1's outputs – was surprisingly effective. This highlighted the power of distillation in this context.
Alibaba also released reasoning versions of its Qwen LLMs in November 2024.
In December 2024, Google introduced Deep Research in Gemini, a feature in Gemini that conducts multi-step research tasks.
On December 16, 2024, an experiment using a Llama 3B model demonstrated that by scaling test-time compute, a relatively small model could outperform a much larger Llama 70B model on challenging reasoning tasks. This result highlighted that improved inference strategies can unlock latent reasoning capabilities even in compact models.
In January 2025, DeepSeek released R1, a model competitive with o1 at lower cost, highlighting the effectiveness of Group Relative Policy Optimization (GRPO). On January 25, 2025, DeepSeek launched a feature in their DeepSeek R1 model, enabling the simultaneous use of search and reasoning capabilities, which allows for more efficient integration of data retrieval with reflective reasoning processes. OpenAI subsequently released o3-mini, followed by Deep Research which is based on o3. The power of distillation was further demonstrated by s1-32B, achieving strong performance with budget forcing and scaling techniques.
On February 2, 2025, OpenAI released Deep Research,[10] a tool that integrates reasoning and web search in a unified workflow, allowing users to perform complex research tasks that require multi-step reasoning and data synthesis from multiple sources. It is based on o3 and can take from 5 to 30 minutes to generate comprehensive reports.[11]
A large language model (LLM) can be finetuned on a dataset of reasoning tasks with example solutions and reasoning traces. The fine-tuned model can then produce its own reasoning traces for new problems.[12][13]
As it is expensive to get humans to write reasoning traces for a SFT dataset, researchers have proposed ways to automatically construct SFT datasets. In rejection sampling finetuning (RFT), new reasoning traces are collected via a loop:[14]
A pretrained language model can be further trained by RL. In the RL formalism, a generative language model is a policy 
. A prompt specifying a task to solve is an environmental state 
, and the response of the language model to the prompt is an action 
. The probability that the language model responds 
 with 
 is 
Training a reasoning language model by RL then consists of constructing a reward model 
 to guide the RL process. Intuitively, a reward model describes how desirable/appropriate/good the response is for the prompt. For reasoning language model, the prompt describes a reasoning task, and the reward would be high if the response solves the task, and low if the response fails to solve the task.
For reasoning language models, the model's response 
 may be broken down into multiple steps, in which case it is written as 
Most recent systems use policy-gradient methods such as Proximal Policy Optimization (PPO) because PPO constrains each policy update with a clipped objective, which stabilises training for very large policies.[15]
Outcome reward model, or outcome-supervised RM (ORM),[12] is a reward model that computes the reward of a step 
 determined by the final answer: 
. They are also called "verifiers".
For tasks with an answer that is easy to verify, such as word problems in math, the outcome reward can simply be binary: 1 if the final answer is correct, and 0 otherwise.[12] If the answer is not easy to verify programmatically, humans can manually label the answers as correct or not, then the labels can be used to finetune a base model that predicts the human label.[13] For other kinds of tasks, such as creative writing, where task performance is not binary true/false, one can train a reward model by finetuning a base model on human ranked preference data, such as used in reinforcement learning from human feedback.[16] A base model can also be finetuned to predict, given a partial thinking trace 
, whether the final answer would be correct or not. This can then be used as a binary reward signal.[12]
The ORM is usually trained via logistic regression, i.e. minimizing cross-entropy loss.[17]
Given a PRM, an ORM can be constructed by multiplying the total process reward during the reasoning trace,[16] or by taking the minimum,[17] or some other method to aggregate the process rewards. DeepSeek used a simple ORM for training the R1 model.[18]
Process reward model, or process-supervised RM (PRM),[12] is a reward model that computes the reward of a step 
 determined by the steps so far: 
Given a partial thinking trace 
, a human can be queried as to whether the steps so far are correct, regardless of whether the ultimate answer would be correct. This can then be used as a binary reward signal. As human labels are expensive, a base model can then be finetuned to predict the human labels.[12] The PRM is usually trained by logistic regression on the human labels, i.e. by minimizing the cross-entropy loss between the true labels and the predicted labels.[17]
As an example, in a 2023 OpenAI paper, 800K process labels were collected for 75K solution traces. A labeler would be presented with a solution trace, and keep labelling "positive" if the step progresses towards the solution, "neutral" if it is not wrong, but does not progress towards solution, and "negative" if it is a mistake. As soon as a "negative" label is entered, the labeler stops labeling that thinking trace, and begins labeling another one. The idea was that, while labelling subsequent reasoning steps can provide even richer supervision signals, simply labeling up to the first error was sufficient for training a competent PRM.[16][19]
As human labels are expensive, researchers have proposed methods to create PRM without human labels on the processes. Inspired by Monte Carlo tree search (MCTS), the Math-Shepherd method samples multiple continuations until the end, starting at each reasoning step 
, and set the reward at that step to be either 
(correct answers)
(total answers)
 in the case of "soft estimation", or 
if one of the answers is correct
else
 in the case of "hard estimation". This creates process reward using only an ORM, which is usually easier or cheaper to construct. After creating these process reward labels, a PRM can be trained on them.[17] Some have tried a fully MCTS approach.[20]
One can also use an ORM to implicitly construct a PRM, similar to direct preference optimization.[21]
A trained ORM can be used to select the best response. The policy would rollout multiple responses, and a trained ORM would select the best response. This allows a simple form of test time compute scaling ("best-of-N").[13] [22]
A trained PRM can also be used to guide reasoning by greedy tree search. That is, the policy model generates several possible next reasoning steps, and the PRM selects the best one, and the process repeats. This is similar to how a trained ORM can be used to select the best response.[23] Beam search perform better than greedy search.
Lookahead search is another tree search method, where the policy model generates several possible next reasoning steps, then make a (partial) rollout for each. If a solution endpoint is reached during the forward simulation, the process halts early. Otherwise, the PRM is used to calculate the total reward for each rollout. The step with the highest rollout is selected.[24]
Self-consistency can be combined with an ORM. The model would be used to generate multiple answers, and the answers would be clustered, so that each cluster has the same answer. The ORM is used to compute the reward for each answer, and the rewards within each cluster is summed. The answer corresponding to the cluster with the highest summed reward is output.[17]
Reasoning models generally outperform non-reasoning models in most benchmarks, especially on tasks requiring multi-step reasoning.
However, some benchmarks exclude reflective models due to longer response times.
The HLE, a rigorous benchmark designed to assess expert-level reasoning across mathematics, humanities, and the natural sciences, reveals substantial performance gaps among models. State-of-the-art reasoning models have demonstrated low accuracy on HLE, highlighting significant room for improvement. In particular, the full reasoning model o3 achieved an accuracy of 26.6%,[25] while its lighter counterpart, o3‑mini-high (evaluated on text‑only questions), reached 13%.[26]
The American Invitational Mathematics Examination (AIME) benchmark, a challenging mathematics competition, demonstrates significant performance differences between model types. Non-reasoning models typically solve less than 30% of AIME. In contrast, models employing reasoning techniques score between 50% and 80%.[27] While OpenAI's o1 maintained or slightly improved its accuracy from reported 2024[citation needed] metrics to 2025 AIME results, o3-mini (high) achieved a higher accuracy (80%) at a significantly lower cost (approximately 12 times cheaper).
According to OpenAI's January 2025 report on o3-mini, adjustable "reasoning effort" significantly affects performance, particularly in STEM. Increasing reasoning effort from low to high boosts accuracy on benchmarks like AIME 2024, GPQA Diamond, and Codeforces, providing performance gains typically in the range of 10-30%. With high reasoning effort, o3-mini (high) achieved 87.3% in AIME (different from the MathArena AIME benchmark results), 79.7% in GPQA Diamond, 2130 Elo in Codeforces, and 49.3 in SWE-bench Verified.[28]
Reasoning models require significantly more test-time compute than non-reasoning models. On the AIME benchmark, reasoning models were 10 to 74 times more expensive[16] than non-reasoning counterparts.
Reflective reasoning increases response times, with current models taking anywhere from three seconds to several minutes to generate an answer. As reasoning depth improves, future models may require even longer processing times.
